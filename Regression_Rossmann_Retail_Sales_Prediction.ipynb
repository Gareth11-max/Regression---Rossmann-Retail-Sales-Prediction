{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "448CDAPjqfQr",
        "y-Ehk30pYrdP",
        "GwzvFGzlYuh3",
        "_ouA3fa0phqN",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "JMzcOPDDphqR",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "fF3858GYyt-u",
        "Fd15vwWVpUZf",
        "dWbDXHzopZyI",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "rAdphbQ9Bhjc",
        "T5CmagL3EC8N",
        "TIqpNgepFxVj",
        "zVGeBEFhpsJ2",
        "Z-hykwinpx6N",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gareth11-max/Regression---Rossmann-Retail-Sales-Prediction/blob/main/Regression_Rossmann_Retail_Sales_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Rossmann Retail Sales Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Rossmann Retail Sales Prediction project involves developing a machine learning model to forecast daily sales for over 1,100 Rossmann stores located in 7 European countries. The task is to predict sales for up to six weeks in advance based on historical data, considering various influencing factors such as promotions, holidays, competition, store location, and seasonality\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann, a leading drug store chain operating over 3,000 stores in 7 European countries, faces the challenge of accurately predicting daily sales for its stores. Store managers are tasked with forecasting sales up to six weeks in advance to optimize inventory management, staffing, promotions, and other operational aspects. However, sales are influenced by numerous factors such as promotions, state and school holidays, competition, seasonality, and store-specific conditions. The problem is further complicated by some stores being temporarily closed for refurbishment.\n",
        "\n",
        "The objective of this project is to develop a predictive model that accurately forecasts daily sales for 1,115 Rossmann stores using historical sales data. The model should consider multiple factors that impact sales, such as:\n",
        "\n",
        "Promotions: Ongoing promotional campaigns that could affect daily sales.\n",
        "Competitor Proximity: Distance to nearby competing stores, which could influence sales.\n",
        "Holiday Information: Effects of state holidays and school holidays on shopping behavior.\n",
        "Store-Specific Factors: Different store types and their impact on sales.\n",
        "Store Closures: Temporary closures for store refurbishments and their effects on sales.\n",
        "The target variable is the Sales column, and the goal is to build a regression model capable of predicting sales for the test set. This will enable Rossmann to forecast sales with higher accuracy, facilitating better planning for inventory, staff, and promotions, and ultimately improving operational efficiency and profitability."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "train_df=pd.read_csv(\"/content/Rossmann Stores Data.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "train_shape = train_df.shape\n",
        "train_shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "train_duplicates = train_df.duplicated().sum()\n",
        "train_duplicates"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "train_missing_values = train_df.isnull().sum()\n",
        "train_missing_values"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize missing values in the training dataset\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "plt.title(\"Missing Values Heatmap for Training Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains no null or duplicate values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "train_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df_cleaned = train_df.drop_duplicates()\n",
        "print(f\"\\nDataset after removing duplicates has {df_cleaned.shape[0]} rows.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaned the dataset to prerpare it for analysis\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sales trend over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=train_df, x='Date', y='Sales')\n",
        "plt.title('Sales Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show the sales trend over time"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales has fluctuated but the trend is the same."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Sales distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(train_df['Sales'], kde=True)\n",
        "plt.title('Sales Distribution')\n",
        "plt.xlabel('Sales')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show sale distribution."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sale in the range 10,000 to 20,000 has the highest frequency"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram can show the distribution of sales, which helps in understanding its spread and central tendency."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Sales by day of the week\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='DayOfWeek', y='Sales', data=train_df)\n",
        "plt.title('Sales by Day of the Week')\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot helps identify how sales vary by the day of the week."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average sale on first day of the week is the highest"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "can be used to improve on days with less average sale."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Sales by month\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Month', y='Sales', data= train_df)\n",
        "plt.title('Sales by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help us analyze if thereâ€™s any seasonality based on months."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain months have higher sales."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Sales by store type\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Store', y='Sales', data=train_df)\n",
        "plt.title('Sales by Store Type')\n",
        "plt.xlabel('Store Type')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "VCwtlGqwYR53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart will show sales by store type, helping identify if certain store types perform better."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be helpful in determining future investments."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Customers', y='Sales', data=train_df)\n",
        "plt.title('Sales by Customer')\n",
        "plt.xlabel('customer')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show sale by customer"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will be helpful in creating a database for most profitable customers."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "top_stores = train_df.groupby('Store')['Sales'].mean().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_stores.plot(kind='bar')\n",
        "plt.title('Top 10 Stores by Average Sales')\n",
        "plt.xlabel('Store')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To show top 10 stores."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store 262 is the top store by sales."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can be used to track important stores."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Promo', y='Sales', data=train_df)\n",
        "plt.title('Sales by Promotion (Promo vs Non-Promo)')\n",
        "plt.xlabel('Promotion')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot helps compare the sales on promotional days vs non-promotional days.\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='StateHoliday', y='Sales', data=train_df)\n",
        "plt.title('Sales by State Holiday')\n",
        "plt.xlabel('State Holiday')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows how sales are affected by state holidays."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales are heavily affected on holidays."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales are dropping on holidays."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='SchoolHoliday', y='Sales', data=train_df)\n",
        "plt.title('Sales by School Holiday')\n",
        "plt.xlabel('School Holiday')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare sales during school holidays and non-school holidays."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much differnce in average sales during school and non school days"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Open', y='Sales', data=train_df)\n",
        "plt.title('Sales by Open vs Closed Stores')\n",
        "plt.xlabel('Open (1 = Open, 0 = Closed)')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot compares sales between open and closed stores."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "train_df['WeekdayOrWeekend'] = train_df['DayOfWeek'].apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='WeekdayOrWeekend', y='Sales', data=train_df)\n",
        "plt.title('Sales by Weekday vs Weekend')\n",
        "plt.xlabel('Weekday or Weekend')\n",
        "plt.ylabel('Sales')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot compares sales between weekdays and weekends."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average sales on weekends is more"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant numerical columns for correlation matrix\n",
        "correlation_data = train_df[['Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday']]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Add a title to the heatmap\n",
        "plt.title('Correlation Heatmap of Numerical Variables')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statements:\n",
        "Statement 1: Stores that run promotions (Promo = 1) have higher average sales compared to stores that don't run promotions (Promo = 0).\n",
        "Statement 2: Sales are higher during weekends (DayOfWeek >= 5) than on weekdays (DayOfWeek < 5).\n",
        "Statement 3: The number of customers is positively correlated with sales, meaning as the number of customers increases, sales also increase."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Hypothesis:\n",
        "Null Hypothesis (Hâ‚€): There is no difference in the average sales between stores that run promotions and stores that don't run promotions.\n",
        "ð»\n",
        "0\n",
        ":\n",
        "ðœ‡\n",
        "PromoÂ =Â 1\n",
        "=\n",
        "ðœ‡\n",
        "PromoÂ =Â 0\n",
        "H\n",
        "0\n",
        "â€‹\n",
        " :Î¼\n",
        "PromoÂ =Â 1\n",
        "â€‹\n",
        " =Î¼\n",
        "PromoÂ =Â 0\n",
        "â€‹\n",
        "\n",
        "Alternative Hypothesis (Hâ‚): Stores that run promotions (Promo = 1) have higher average sales compared to stores that don't run promotions (Promo = 0).\n",
        "ð»\n",
        "1\n",
        ":\n",
        "ðœ‡\n",
        "PromoÂ =Â 1\n",
        ">\n",
        "ðœ‡\n",
        "PromoÂ =Â 0\n",
        "H\n",
        "1\n",
        "â€‹\n",
        " :Î¼\n",
        "PromoÂ =Â 1\n",
        "â€‹\n",
        " >Î¼\n",
        "PromoÂ =Â 0\n",
        "â€‹\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "# Split data into two groups: Promo = 1 (stores with promotions) and Promo = 0 (stores without promotions)\n",
        "promo_sales = train_df[train_df['Promo'] == 1]['Sales']\n",
        "no_promo_sales = train_df[train_df['Promo'] == 0]['Sales']\n",
        "\n",
        "# Perform a one-tailed t-test (Promo = 1 vs Promo = 0)\n",
        "t_stat, p_value = stats.ttest_ind(promo_sales, no_promo_sales, alternative='greater')\n",
        "\n",
        "# Display the results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Determine if we can reject the null hypothesis at 5% significance level\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. Stores with promotions have higher sales on average.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. No significant difference in average sales between stores with and without promotions.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test that was used to obtain the p-value is the Independent Two-Sample t-test (also known as the Student's t-test)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€): There is no difference in the average sales between weekends and weekdays.\n",
        "ð»\n",
        "0\n",
        ":\n",
        "ðœ‡\n",
        "Weekend\n",
        "=\n",
        "ðœ‡\n",
        "Weekday\n",
        "H\n",
        "0\n",
        "â€‹\n",
        " :Î¼\n",
        "Weekend\n",
        "â€‹\n",
        " =Î¼\n",
        "Weekday\n",
        "â€‹\n",
        "\n",
        "Alternative Hypothesis (Hâ‚): Sales during weekends are higher than sales during weekdays.\n",
        "ð»\n",
        "1\n",
        ":\n",
        "ðœ‡\n",
        "Weekend\n",
        ">\n",
        "ðœ‡\n",
        "Weekday\n",
        "H\n",
        "1\n",
        "â€‹\n",
        " :Î¼\n",
        "Weekend\n",
        "â€‹\n",
        " >Î¼\n",
        "Weekday\n",
        "â€‹\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Split data into weekends and weekdays\n",
        "weekend_sales = train_df[train_df['DayOfWeek'] >= 5]['Sales']\n",
        "weekday_sales = train_df[train_df['DayOfWeek'] < 5]['Sales']\n",
        "\n",
        "# Perform a one-tailed t-test (Weekends vs Weekdays)\n",
        "t_stat, p_value = stats.ttest_ind(weekend_sales, weekday_sales, alternative='greater')\n",
        "\n",
        "# Display results\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Determine if we can reject the null hypothesis at 5% significance level\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. Sales are higher during weekends than weekdays.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. No significant difference in sales between weekends and weekdays.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test that was used to obtain the p-value is the Independent Two-Sample t-test (also known as the Student's t-test)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€): There is no correlation between the number of customers and sales.\n",
        "\n",
        "ð»\n",
        "0\n",
        ":\n",
        "ðœŒ\n",
        "=\n",
        "0\n",
        "H\n",
        "0\n",
        "â€‹\n",
        " :Ï=0 (where\n",
        "ðœŒ\n",
        "Ï is the correlation coefficient)\n",
        "Alternative Hypothesis (Hâ‚): There is a positive correlation between the number of customers and sales.\n",
        "\n",
        "ð»\n",
        "1\n",
        ":\n",
        "ðœŒ\n",
        ">\n",
        "0\n",
        "H\n",
        "1\n",
        "â€‹\n",
        " :Ï>0"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Calculate the Pearson correlation coefficient\n",
        "correlation, p_value = stats.pearsonr(train_df['Customers'], train_df['Sales'])\n",
        "\n",
        "# Display results\n",
        "print(f\"Correlation: {correlation}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Determine if we can reject the null hypothesis at 5% significance level\n",
        "alpha = 0.05\n",
        "if p_value < alpha and correlation > 0:\n",
        "    print(\"Reject the null hypothesis. There is a positive correlation between the number of customers and sales.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. No significant positive correlation between the number of customers and sales.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test that was used to obtain the p-value is the Independent Two-Sample t-test (also known as the Student's t-test)."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Checking for missing values in the dataset\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_percentage = (train_df.isnull().sum() / len(train_df)) * 100\n",
        "\n",
        "# Display missing values count and percentage\n",
        "print(\"Missing Values Count:\")\n",
        "print(missing_values)\n",
        "print(\"\\nMissing Values Percentage:\")\n",
        "print(missing_percentage)\n",
        "# Impute missing values for numerical columns with the mean or median\n",
        "train_df['Sales'] = train_df['Sales'].fillna(train_df['Sales'].mean())  # Mean imputation for 'Sales'\n",
        "train_df['Customers'] = train_df['Customers'].fillna(train_df['Customers'].median())  # Median imputation for 'Customers'\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean imputation and median imputation."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "g7kNnDxrCNkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "sns.boxplot(x=train_df['Sales'])\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "z_scores = np.abs(stats.zscore(train_df['Sales']))  # Replace 'sales' with relevant column\n",
        "df_outliers = train_df[z_scores > 3]  # Outliers with Z-score > 3\n",
        "\n",
        "Q1 = train_df['Sales'].quantile(0.25)\n",
        "Q3 = train_df['Sales'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df_outliers = train_df[(train_df['Sales'] < (Q1 - 1.5 * IQR)) | (train_df['Sales'] > (Q3 + 1.5 * IQR))]\n",
        "\n",
        "\n",
        "df_cleaned = train_df[(train_df['Sales'] >= (Q1 - 1.5 * IQR)) & (train_df['Sales'] <= (Q3 + 1.5 * IQR))]\n",
        "\n",
        "df_cleaned\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IQR method is robust because it focuses on the spread of the middle 50% of the data, which helps mitigate the impact of extreme values on the identification of outliers.\n",
        "\n",
        "The Z-score is useful when the data follows a normal (Gaussian) distribution because it assumes that most data points lie within a certain number of standard deviations from the mean."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# 1. Handle missing values (if any)\n",
        "train_df['StateHoliday'].fillna('0', inplace=True)  # Assuming missing values in StateHoliday should be treated as '0'\n",
        "\n",
        "# 2. Date feature extraction\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])  # Convert Date column to datetime\n",
        "train_df['Year'] = train_df['Date'].dt.year\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding transforms each unique category into a numeric value while retaining the ordinal relationship. For example, Monday becomes 0, Tuesday becomes 1, and so on.\n",
        "\n",
        "The Store and StateHoliday columns are nominal variables because they represent categories with no inherent order or ranking. For example, Store indicates different store IDs (each store is independent of others), and StateHoliday represents different holiday types (like '0', 'a', 'b')."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "\n",
        "# Define a dictionary for contractions and their expansions\n",
        "contractions_dict = {\n",
        "    \"I'm\": \"I am\", \"I've\": \"I have\", \"I'd\": \"I would\", \"I'll\": \"I will\", \"I am\": \"I am\",\n",
        "    \"you're\": \"you are\", \"you've\": \"you have\", \"you'll\": \"you will\", \"you'd\": \"you would\",\n",
        "    \"he's\": \"he is\", \"he'll\": \"he will\", \"he'd\": \"he would\", \"she's\": \"she is\", \"she'll\": \"she will\",\n",
        "    \"she'd\": \"she would\", \"it's\": \"it is\", \"it'll\": \"it will\", \"it'd\": \"it would\",\n",
        "    \"we're\": \"we are\", \"we've\": \"we have\", \"we'll\": \"we will\", \"we'd\": \"we would\",\n",
        "    \"they're\": \"they are\", \"they've\": \"they have\", \"they'll\": \"they will\", \"they'd\": \"they would\",\n",
        "    \"can't\": \"cannot\", \"won't\": \"will not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
        "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
        "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"shouldn't\": \"should not\",\n",
        "    \"couldn't\": \"could not\", \"wouldn't\": \"would not\", \"mustn't\": \"must not\", \"mightn't\": \"might not\",\n",
        "    \"ain't\": \"is not\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions in a given text\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    # Define a regular expression pattern to match contractions\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
        "\n",
        "    # Replace contractions using the dictionary\n",
        "    expanded_text = pattern.sub(lambda x: contractions_dict[x.group(0)], text)\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"I'm happy that you're coming over today! It's going to be fun.\"\n",
        "\n",
        "expanded_text = expand_contractions(sample_text, contractions_dict)\n",
        "\n",
        "print(f\"Original Text: {sample_text}\")\n",
        "print(f\"Expanded Text: {expanded_text}\")\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Example text\n",
        "sample_text = \"This is a Simple Text with Mixed CASE letters!\"\n",
        "\n",
        "# Convert the text to lowercase\n",
        "lowercased_text = sample_text.lower()\n",
        "\n",
        "print(f\"Original Text: {sample_text}\")\n",
        "print(f\"Lowercased Text: {lowercased_text}\")\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Example text with punctuation\n",
        "sample_text = \"Hello there! How are you doing today? I hope you're doing great.\"\n",
        "\n",
        "# Remove punctuation using string.punctuation\n",
        "cleaned_text = ''.join(char for char in sample_text if char not in string.punctuation)\n",
        "\n",
        "print(f\"Original Text: {sample_text}\")\n",
        "print(f\"Text without Punctuation: {cleaned_text}\")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Function to remove URLs and words containing digits\n",
        "def clean_text(text):\n",
        "    # Remove URLs (matches anything starting with http:// or https:// and any domain)\n",
        "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "\n",
        "    # Remove words containing digits (e.g., 'hello123', 'product_2021')\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example text with URLs and words containing digits\n",
        "sample_text = \"\"\"\n",
        "Visit our website at https://www.example.com for more info.\n",
        "Our new product version is available: product_2021.\n",
        "Contact us at support@example\"\"\"\n",
        "\n",
        "cleaned_text = clean_text(sample_text)\n",
        "cleaned_text\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "sample_text = \"This is an example sentence where some stopwords will be removed.\"\n",
        "\n",
        "# Get English stopwords list from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text (split it into words)\n",
        "words = sample_text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Join words back into a clean sentence\n",
        "cleaned_text = ' '.join(filtered_words)\n",
        "\n",
        "print(f\"Original Text: {sample_text}\")\n",
        "print(f\"Text without Stopwords: {cleaned_text}\")\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained model for paraphrasing\n",
        "paraphrase_model = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "\n",
        "# Example text to rephrase\n",
        "text = \"I enjoy going to the park in the evening because it is relaxing.\"\n",
        "\n",
        "# Generate a paraphrase\n",
        "paraphrased_text = paraphrase_model(f\"paraphrase: {text}\", max_length=50, num_return_sequences=1)\n",
        "\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Paraphrased Text: {paraphrased_text[0]['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download punkt tokenizer models if not already installed\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example text\n",
        "sample_text = \"I love Natural Language Processing!\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "print(f\"Tokens: {tokens}\")\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "sample_text = \"This is a sample sentence with stopwords.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = sample_text.split()\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "normalized_text = \" \".join(filtered_tokens)\n",
        "\n",
        "print(normalized_text)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sample_text = \"running runs runner\"\n",
        "tokens = sample_text.split()\n",
        "\n",
        "# Apply stemming to each token\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "normalized_text = \" \".join(stemmed_tokens)\n",
        "\n",
        "print(normalized_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming reduces words to their root form, which may not always be a valid word but helps in grouping different forms of a word.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Standardize the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = train_df.drop('Sales', axis=1)  # Assuming 'target' is your dependent variable\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce dimensionality and decorrelate features\n",
        "pca = PCA(n_components=2)  # Reduce to 2 components, for example\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Correlation matrix for numerical features\n",
        "corr_matrix = train_df.corr()\n",
        "\n",
        "# Plot heatmap for correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated features (correlation > 0.9)\n",
        "threshold = 0.9\n",
        "high_corr_pairs = set()\n",
        "\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            high_corr_pairs.add(colname)\n",
        "\n",
        "# Drop highly correlated features\n",
        "train_df = train_df.drop(columns=high_corr_pairs)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performed scaling,categorical encoding and handling outliers."
      ],
      "metadata": {
        "id": "FV5rSKVhJK5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "NwBwX1R9JnQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume you have numerical features in train_df that need scaling\n",
        "numerical_features = ['Sales']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data (Standardization)\n",
        "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
        "\n",
        "# Check the transformed data\n",
        "print(train_df[numerical_features].head())\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data (Normalization)\n",
        "train_df[numerical_features] = min_max_scaler.fit_transform(train_df[numerical_features])\n",
        "\n",
        "# Check the transformed data\n",
        "print(train_df[numerical_features].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization scales the data within a fixed range, usually between 0 and 1. This is especially useful when the algorithm makes use of distance metrics like KNN or Neural Networks.\n",
        "\n",
        "Standardization involves subtracting the mean and dividing by the standard deviation. This makes the feature distribution have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "tIvTPtXpJ6aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = train_df.drop(columns=['Sales'])\n",
        "y = train_df['Sales']  # Target variable\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape of the split data\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the data splitting, I used an 80/20 ratio, where:\n",
        "\n",
        "80% of the data is used for training the model.\n",
        "\n",
        "20% of the data is reserved for testing the model's performance.\n",
        "\n",
        "An 80/20 split strikes a good balance between having enough data for training while also reserving sufficient data to evaluate the model's generalization to new, unseen data.\n",
        "\n",
        "The training set needs to be large enough to allow the model to learn meaningful patterns, while the test set needs to be large enough to provide a reliable estimate of the model's performance."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If 'Date' is a datetime column, we need to convert it\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "# Extract useful features from the datetime column\n",
        "train_df['Year'] = train_df['Date'].dt.year\n",
        "train_df['Month'] = train_df['Date'].dt.month\n",
        "train_df['Day'] = train_df['Date'].dt.day\n",
        "train_df['Weekday'] = train_df['Date'].dt.weekday  # Monday=0, Sunday=6\n",
        "\n",
        "# Drop the original Date column as it's no longer needed\n",
        "train_df.drop(columns=['Date'], inplace=True)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "\n",
        "# Handle missing values - filling with median\n",
        "train_df.fillna(train_df.median(), inplace=True)\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "train_df = pd.get_dummies(train_df, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = train_df.drop(columns=['Sales'])\n",
        "y = train_df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n"
      ],
      "metadata": {
        "id": "qG-JKMBbRnau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we have used the Random Forest Regressor model to predict the Sales for the Rosman Retail Dataset"
      ],
      "metadata": {
        "id": "57gbaTpfSHnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculating MAE, MSE, RMSE, and RÂ²\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a list of metric names and their corresponding values\n",
        "metrics = ['MAE', 'MSE', 'RÂ²']\n",
        "values = [mae, mse, r2]\n",
        "\n",
        "# Plotting the metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, values, color=['blue', 'orange', 'red'])\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Define the Random Forest model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [10, 20, None],  # Depth of each tree\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at each split\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_grid = grid_search.best_params_\n",
        "print(\"Best Parameters from GridSearchCV:\", best_params_grid)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_grid = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_grid = mean_absolute_error(y_test, y_pred_grid)\n",
        "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "\n",
        "print(f\"GridSearchCV - MAE: {mae_grid}, MSE: {mse_grid}, RÂ²: {r2_grid}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define the Random Forest model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameters for RandomizedSearch\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 300, 50),  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30, 40],  # Depth of each tree\n",
        "    'min_samples_split': [2, 5, 10, 20],  # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 2, 4, 8],  # Minimum samples required at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at each split\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, n_jobs=-1, random_state=42, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params_random = random_search.best_params_\n",
        "print(\"Best Parameters from RandomizedSearchCV:\", best_params_random)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_random = mean_absolute_error(y_test, y_pred_random)\n",
        "mse_random = mean_squared_error(y_test, y_pred_random)\n",
        "r2_random = r2_score(y_test, y_pred_random)\n",
        "\n",
        "print(f\"RandomizedSearchCV - MAE: {mae_random}, MSE: {mse_random}, RÂ²: {r2_random}\")\n",
        "\n",
        "\n",
        "# Predict on the model\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the objective function to minimize\n",
        "def objective(params):\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_depth=params['max_depth'],\n",
        "        min_samples_split=params['min_samples_split'],\n",
        "        min_samples_leaf=params['min_samples_leaf'],\n",
        "        max_features=params['max_features'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the model\n",
        "    score = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
        "    return -score  # Return the negative score since Hyperopt minimizes the objective\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "space = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 150, 200]),\n",
        "    'max_depth': hp.choice('max_depth', [10, 20, 30, None]),\n",
        "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
        "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
        "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2'])\n",
        "}\n",
        "\n",
        "# Set up the trials object to store the results\n",
        "trials = Trials()\n",
        "\n",
        "# Perform the optimization\n",
        "best_params_bayes = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "print(\"Best Parameters from Bayesian Optimization:\", best_params_bayes)\n",
        "\n",
        "# Once the best parameters are found, train the final model\n",
        "best_model_bayes = RandomForestRegressor(\n",
        "    n_estimators=best_params_bayes['n_estimators'],\n",
        "    max_depth=best_params_bayes['max_depth'],\n",
        "    min_samples_split=best_params_bayes['min_samples_split'],\n",
        "    min_samples_leaf=best_params_bayes['min_samples_leaf'],\n",
        "    max_features=best_params_bayes['max_features'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the full training data\n",
        "best_model_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_bayes = best_model_bayes.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_bayes = mean_absolute_error(y_test, y_pred_bayes)\n",
        "mse_bayes = mean_squared_error(y_test, y_pred_bayes)\n",
        "r2_bayes = r2_score(y_test, y_pred_bayes)\n",
        "\n",
        "print(f\"Bayesian Optimization - MAE: {mae_bayes}, MSE: {mse_bayes}, RÂ²: {r2_bayes}\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, Iâ€™ve demonstrated three hyperparameter optimization techniques: GridSearchCV, RandomizedSearchCV, and Bayesian Optimization. The choice of technique largely depends on the trade-off between search space size, computation time, and the level of optimization required."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "id": "y4XKIQ7uEo_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "id": "G509OkfVJH0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Example: Loading your dataset\n",
        "# train_df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Step 1: Check data types and identify columns that are non-numeric\n",
        "print(train_df.dtypes)\n",
        "print(train_df[train_df.isnull().any(axis=1)])  # Rows with NaN values\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Convert 'Date' column to datetime and then to numeric (if 'Date' is present)\n",
        "if 'Date' in train_df.columns:\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'], errors='coerce')  # Convert to datetime\n",
        "    train_df['Date'] = (train_df['Date'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')  # Convert to days since epoch\n",
        "\n",
        "# Step 3: Handle missing values - fill missing numeric columns with median\n",
        "# First, convert all numeric columns explicitly to avoid any string columns\n",
        "numeric_columns = train_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "train_df[numeric_columns] = train_df[numeric_columns].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
        "\n",
        "# Fill missing values with median for numeric columns\n",
        "train_df.fillna(train_df.median(), inplace=True)\n",
        "\n",
        "# Step 4: One-Hot Encoding for categorical columns\n",
        "# Identifying categorical columns\n",
        "categorical_columns = ['Store','DayOfWeek','StateHoliday','SchoolHoliday']\n",
        "\n",
        "# Apply One-Hot Encoding to categorical columns\n",
        "train_df = pd.get_dummies(train_df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Step 5: Define features (X) and target (y)\n",
        "X = train_df.drop(columns=['Sales'])  # Features, drop the target column 'Sales'\n",
        "y = train_df['Sales']  # Target variable 'Sales'\n",
        "\n",
        "# Step 6: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Initialize and train the RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n"
      ],
      "metadata": {
        "id": "qJdkNK8MEM6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Regressor is an ensemble machine learning model used for regression tasks. It works by building multiple decision trees and combining their predictions to make a final prediction. Random Forests use the principle of bagging (Bootstrap Aggregating), where multiple models (trees) are trained on random subsets of the data, and each tree makes an independent prediction. The final prediction is then averaged to reduce overfitting and variance.\n",
        "\n",
        "In this case, the model is used to predict the Sales column based on various features like Store, DayOfWeek, StateHoliday, and others."
      ],
      "metadata": {
        "id": "nE5KDnvfKF3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Define the features (X) and target (y)\n",
        "X = train_df.drop(columns=['Sales'])\n",
        "y = train_df['Sales']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the RandomForestRegressor model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameters grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters from GridSearchCV\n",
        "best_params_grid = grid_search.best_params_\n",
        "print(\"Best Parameters from GridSearchCV:\", best_params_grid)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_grid = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_grid = mean_absolute_error(y_test, y_pred_grid)\n",
        "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "\n",
        "print(f\"GridSearchCV - MAE: {mae_grid}, MSE: {mse_grid}, RÂ²: {r2_grid}\")\n",
        "\n",
        "# Define the RandomizedSearchCV model\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 300, 50),\n",
        "    'max_depth': [None, 10, 20, 30, 40],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, n_jobs=-1, random_state=42, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters from RandomizedSearchCV\n",
        "best_params_random = random_search.best_params_\n",
        "print(\"Best Parameters from RandomizedSearchCV:\", best_params_random)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_random = mean_absolute_error(y_test, y_pred_random)\n",
        "mse_random = mean_squared_error(y_test, y_pred_random)\n",
        "r2_random = r2_score(y_test, y_pred_random)\n",
        "\n",
        "print(f\"RandomizedSearchCV - MAE: {mae_random}, MSE: {mse_random}, RÂ²: {r2_random}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "# Define the objective function to minimize\n",
        "def objective(params):\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        max_depth=params['max_depth'],\n",
        "        min_samples_split=params['min_samples_split'],\n",
        "        min_samples_leaf=params['min_samples_leaf'],\n",
        "        max_features=params['max_features'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the model\n",
        "    score = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
        "    return -score  # Return the negative score since Hyperopt minimizes the objective\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "space = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 150, 200]),\n",
        "    'max_depth': hp.choice('max_depth', [10, 20, 30, None]),\n",
        "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
        "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
        "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2'])\n",
        "}\n",
        "\n",
        "# Set up the trials object to store the results\n",
        "trials = Trials()\n",
        "\n",
        "# Perform the optimization\n",
        "best_params_bayes = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "print(\"Best Parameters from Bayesian Optimization:\", best_params_bayes)\n",
        "\n",
        "# Once the best parameters are found, train the final model\n",
        "best_model_bayes = RandomForestRegressor(\n",
        "    n_estimators=best_params_bayes['n_estimators'],\n",
        "    max_depth=best_params_bayes['max_depth'],\n",
        "    min_samples_split=best_params_bayes['min_samples_split'],\n",
        "    min_samples_leaf=best_params_bayes['min_samples_leaf'],\n",
        "    max_features=best_params_bayes['max_features'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the full training data\n",
        "best_model_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_bayes = best_model_bayes.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_bayes = mean_absolute_error(y_test, y_pred_bayes)\n",
        "mse_bayes = mean_squared_error(y_test, y_pred_bayes)\n",
        "r2_bayes = r2_score(y_test, y_pred_bayes)\n",
        "\n",
        "print(f\"Bayesian Optimization - MAE: {mae_bayes}, MSE: {mse_bayes}, RÂ²: {r2_bayes}\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluation scores for comparison\n",
        "metrics = ['MAE', 'MSE', 'RÂ²']\n",
        "grid_scores = [mae_grid, mse_grid, r2_grid]\n",
        "random_scores = [mae_random, mse_random, r2_random]\n",
        "bayes_scores = [mae_bayes, mse_bayes, r2_bayes]\n",
        "\n",
        "# Create subplots to display evaluation metrics comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Plotting each model's scores\n",
        "ax.bar([f\"{metric} - GridSearchCV\" for metric in metrics], grid_scores, color='blue', alpha=0.7, label='GridSearchCV')\n",
        "ax.bar([f\"{metric} - RandomSearchCV\" for metric in metrics], random_scores, color='green', alpha=0.7, label='RandomSearchCV')\n",
        "ax.bar([f\"{metric} - Bayesian\" for metric in metrics], bayes_scores, color='orange', alpha=0.7, label='Bayesian Optimization')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Evaluation Metrics')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Comparison: GridSearchCV vs RandomSearchCV vs Bayesian Optimization')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code provided, I implemented three hyperparameter optimization techniques:\n",
        "\n",
        "GridSearchCV\n",
        "\n",
        "RandomizedSearchCV\n",
        "\n",
        "Bayesian Optimization (using Hyperopt)\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower MAE and MSE lead to more accurate and reliable predictions of sales, reducing the risk of poor decision-making in inventory management, resource allocation, and revenue forecasting.\n",
        "\n",
        "Higher RÂ² means the model better explains the variations in sales, allowing businesses to identify trends, optimize marketing strategies, and tailor product offerings.\n",
        "\n",
        "By improving these evaluation metrics (lower MAE, MSE, and higher RÂ²), the Random Forest Regressor model can significantly enhance the business's ability to make data-driven, informed decisions, leading to cost savings, increased profits, and better customer satisfaction."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation using Gradient Boosting Regressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Example: Assuming you have a dataframe named train_df with 'Sales' as the target variable\n",
        "\n",
        "# Handle missing values - filling with median\n",
        "train_df.fillna(train_df.median(), inplace=True)\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "train_df = pd.get_dummies(train_df, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = train_df.drop(columns=['Sales'])\n",
        "y = train_df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n",
        "\n",
        "# Optionally, you can also plot the feature importance to interpret the model\n",
        "import matplotlib.pyplot as plt\n",
        "feature_importances = model.feature_importances_\n",
        "sorted_idx = feature_importances.argsort()\n",
        "\n",
        "plt.barh(train_df.drop(columns=['Sales']).columns[sorted_idx], feature_importances[sorted_idx])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Feature Importances in Gradient Boosting Regressor\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluation metrics\n",
        "metrics = ['MAE', 'MSE', 'RÂ²']\n",
        "scores = [mae, mse, r2]\n",
        "\n",
        "# Create a bar chart to visualize the metrics\n",
        "plt.bar(metrics, scores, color=['blue', 'green', 'orange'])\n",
        "plt.xlabel('Evaluation Metrics')\n",
        "plt.ylabel('Scores')\n",
        "plt.title('Model Performance using Gradient Boosting Regressor')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame `train_df` with the 'Sales' column being the target variable\n",
        "\n",
        "# Handle missing values - filling with median\n",
        "train_df.fillna(train_df.median(), inplace=True)\n",
        "\n",
        "# One-Hot Encoding for categorical variables\n",
        "train_df = pd.get_dummies(train_df, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = train_df.drop(columns=['Sales'])\n",
        "y = train_df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the GradientBoostingRegressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (RÂ²): {r2}\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params_grid = grid_search.best_params_\n",
        "print(\"Best Parameters from GridSearchCV:\", best_params_grid)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_grid = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae_grid = mean_absolute_error(y_test, y_pred_grid)\n",
        "print(f\"GridSearchCV - MAE: {mae_grid}\")\n",
        "\n",
        "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
        "print(f\"GridSearchCV - MSE: {mse_grid}\")\n",
        "\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "print(f\"GridSearchCV - RÂ²: {r2_grid}\")\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 300, 50),\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Set up RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, n_jobs=-1, random_state=42, scoring='neg_mean_absolute_error')\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params_random = random_search.best_params_\n",
        "print(\"Best Parameters from RandomizedSearchCV:\", best_params_random)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_random = random_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae_random = mean_absolute_error(y_test, y_pred_random)\n",
        "print(f\"RandomizedSearchCV - MAE: {mae_random}\")\n",
        "\n",
        "mse_random = mean_squared_error(y_test, y_pred_random)\n",
        "print(f\"RandomizedSearchCV - MSE: {mse_random}\")\n",
        "\n",
        "r2_random = r2_score(y_test, y_pred_random)\n",
        "print(f\"RandomizedSearchCV - RÂ²: {r2_random}\")\n",
        "\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the objective function for Bayesian Optimization\n",
        "def objective(params):\n",
        "    model = GradientBoostingRegressor(\n",
        "        n_estimators=params['n_estimators'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=params['max_depth'],\n",
        "        min_samples_split=params['min_samples_split'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Use cross-validation to evaluate the model\n",
        "    score = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=5).mean()\n",
        "    return -score  # Return the negative score as hyperopt minimizes the objective\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "space = {\n",
        "    'n_estimators': hp.choice('n_estimators', [50, 100, 150, 200]),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
        "    'max_depth': hp.choice('max_depth', [3, 4, 5, 6]),\n",
        "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10, 20]),\n",
        "}\n",
        "\n",
        "# Set up the trials object to store the results\n",
        "trials = Trials()\n",
        "\n",
        "# Perform the optimization\n",
        "best_params_bayes = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "\n",
        "print(\"Best Parameters from Bayesian Optimization:\", best_params_bayes)\n",
        "\n",
        "# Once the best parameters are found, train the final model\n",
        "best_model_bayes = GradientBoostingRegressor(\n",
        "    n_estimators=best_params_bayes['n_estimators'],\n",
        "    learning_rate=best_params_bayes['learning_rate'],\n",
        "    max_depth=best_params_bayes['max_depth'],\n",
        "    min_samples_split=best_params_bayes['min_samples_split'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model on the full training data\n",
        "best_model_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_bayes = best_model_bayes.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mae_bayes = mean_absolute_error(y_test, y_pred_bayes)\n",
        "print(f\"Bayesian Optimization - MAE: {mae_bayes}\")\n",
        "\n",
        "mse_bayes = mean_squared_error(y_test, y_pred_bayes)\n",
        "print(f\"Bayesian Optimization - MSE: {mse_bayes}\")\n",
        "\n",
        "r2_bayes = r2_score(y_test, y_pred_bayes)\n",
        "print(f\"Bayesian Optimization - RÂ²: {r2_bayes}\")\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we used these techniques is that each has its own advantages depending on the size of the hyperparameter space, the computational resources available, and the time constraints. For exhaustive searches in smaller spaces, GridSearchCV is ideal. For larger spaces with less computational expense, RandomizedSearchCV is more efficient. For highly efficient optimization with fewer evaluations, Bayesian Optimization is the most suitable choice."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE is chosen because it gives a clear and direct interpretation of how far off the model's predictions are from actual values, which is crucial for practical decision-making (like predicting sales revenue).\n",
        "\n",
        "MSE is useful for penalizing large errors more heavily. In business contexts, errors like overstocking or understocking products can have disproportionately high costs. Therefore, minimizing MSE helps prevent these costly mistakes.\n",
        "\n",
        "RÂ² is valuable because it reflects how much of the variation in the target variable is captured by the model. In business, models with high RÂ² values provide confidence that the predictions are based on strong relationships within the data, which can guide strategic decisions like marketing spends, product demand forecasting, and resource allocation."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this problem, the final model would likely be the Random Forest Regressor or XGBoost depending on the dataset's size, complexity, and the need for interpretability.\n",
        "\n",
        "Why Random Forest Regressor?\n",
        "\n",
        "Consistency and Stability: Random Forest is a robust and stable model. It is particularly useful when the dataset has a mix of numerical and categorical features and when the relationships between features are complex and non-linear.\n",
        "\n",
        "Resilience to Overfitting: Random Forest generally performs well even when the model is overfitting on certain training sets, as the ensemble method reduces overfitting by averaging predictions from different decision trees.\n",
        "\n",
        "Good Performance: Random Forest can handle both small and large datasets effectively, and it was likely chosen if the data had many features and needed robust handling of overfitting.\n",
        "\n",
        "Business Use Case: If the focus is on forecasting sales, inventory, or predicting demand for a product, Random Forest can offer a solid prediction, where accuracy and reliable decision-making are key.\n",
        "\n",
        "Why XGBoost (if chosen)?\n",
        "\n",
        "Predictive Power: If the dataset is large and complex, and the goal is to maximize predictive accuracy, XGBoost might be chosen as the final model. It is known to produce superior results due to its gradient boosting technique and ability to fine-tune parameters for better performance.\n",
        "\n",
        "Handling Imbalanced Data: XGBoost is particularly useful when dealing with imbalanced datasets or datasets with many outliers."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: Random Forest Regressor\n",
        "Random Forest Regressor is an ensemble learning method that constructs multiple decision trees during training and outputs the average of their predictions for regression tasks. Each tree in the forest is built by considering a subset of the features and data points (bootstrap sampling). This allows it to model complex relationships while reducing overfitting."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we employed a Random Forest Regressor model for predictive analysis, primarily focused on predicting target variables such as sales or demand. Random Forest, being an ensemble method, has demonstrated its strength in handling complex, non-linear relationships while maintaining robustness against overfitting. This makes it a reliable choice for real-world business applications, where the data is often noisy and intricate.\n",
        "\n",
        "By leveraging hyperparameter optimization techniques like GridSearchCV, RandomizedSearchCV, and Bayesian Optimization, we fine-tuned the model to achieve the best possible performance, ensuring accuracy and generalization. These optimization methods allowed us to find the optimal set of parameters, significantly enhancing the model's predictive capabilities."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}